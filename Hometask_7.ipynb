{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "070066ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c6e5981",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "nameOfTag = newsgroups_train.target_names\n",
    "n_features = 4700\n",
    "n_components = 20\n",
    "n_top_words = 10\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "                    lowercase=True, stop_words=_stop_words.ENGLISH_STOP_WORDS,\n",
    "                    analyzer='word', binary=True,\n",
    "                    max_df=0.95, min_df=2,\n",
    "                    max_features=n_features\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58d1a423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _customLDA(n_d_k, n_k_w, n_k, _z, _document, _word, _alpha, _beta, _topic,  max_iter=10):\n",
    "    for i in tqdm(range(max_iter)):\n",
    "        for j in range(len(_document)):\n",
    "            cur_word = _word[j]\n",
    "            cur_document = _document[j]\n",
    "            cur_topic = _z[j]\n",
    "            n_d_k[cur_document, cur_topic] -= 1\n",
    "            n_k_w[cur_topic, cur_word] -= 1\n",
    "            n_k[cur_topic] -= 1\n",
    "            p = (n_d_k[cur_document, :] + _alpha) * (n_k_w[:, cur_word] + _beta[cur_word]) / (n_k + _beta.sum())\n",
    "            _z[j] = np.random.choice(np.arange(_topic), p = p / p.sum())\n",
    "            n_d_k[cur_document, _z[j]] += 1\n",
    "            n_k_w[_z[j], cur_word] += 1\n",
    "            n_k[_z[j]] += 1\n",
    "    return n_d_k, n_k_w, n_k, _z\n",
    "topic = 20\n",
    "n_d_k = np.zeros( topic * X_train.shape[0]).reshape(X_train.shape[0], topic)\n",
    "n_k_w = np.zeros( topic * X_train.shape[1]).reshape(topic, X_train.shape[1])\n",
    "n_k = np.zeros(topic)\n",
    "document, word = X_train.nonzero()\n",
    "z = np.random.choice(topic, len(document))\n",
    "for i, j, k in zip(document, word, z):\n",
    "    n_d_k[i, k] += 1\n",
    "    n_k_w[k, j] += 1\n",
    "    n_k[k] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb3ff03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [24:44<00:00, 49.47s/it]\n"
     ]
    }
   ],
   "source": [
    "n_d_k, n_k_w,  n_k, z = _customLDA(n_d_k, n_k_w, n_k, z, document, word, np.ones(20), np.ones(X_train.shape[1]), 20, max_iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "131b722e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag 1 \tdon\tjust\tknow\tlike\tmake\tpeople\treally\tsay\tthings\tthink\n",
      "Tag 2 \tcode\tend\tnumber\tposting\tproblem\tquestions\ttime\ttry\tuse\tway\n",
      "Tag 3 \tbike\tcar\tdoes\tgood\tjust\tlike\tneed\tpower\twant\twork\n",
      "Tag 4 \tcourse\tdoesn\tdon\tlike\tpeople\tpoint\tprobably\tproblem\tsay\tve\n",
      "Tag 5 \tcondition\tedu\tinterested\tmail\tnew\toffer\tprice\tsale\tsell\tsend\n",
      "Tag 6 \tarticle\tday\tdays\tlater\tlike\tnumber\told\tprobably\ttimes\tyear\n",
      "Tag 7 \tdid\tdidn\tgoing\tjust\told\tright\tsaid\ttime\tve\tway\n",
      "Tag 8 \tdoesn\tdon\tgoing\tjust\tknow\tmake\tthing\tthink\ttime\tve\n",
      "Tag 9 \tcountry\tgovernment\tisrael\tlaw\tpeople\trights\tstate\tstates\twar\tworld\n",
      "Tag 10 \tcost\tdevelopment\tgeneral\tlow\tmajor\tnasa\tnational\tresearch\tscience\tspace\n",
      "Tag 11 \tdoes\tdon\tgood\tjust\tknow\tlike\tlittle\tmake\tnew\twant\n",
      "Tag 12 \tchip\tclipper\tencryption\tgovernment\tkey\tkeys\tpublic\tsecurity\tuse\tused\n",
      "Tag 13 \tlet\tmake\tpeople\tquestion\treally\tright\tthink\ttime\ttrying\tway\n",
      "Tag 14 \taddress\tcom\tedu\temail\tinformation\tknow\tlooking\tmail\tpost\tthanks\n",
      "Tag 15 \tbest\tbetter\tgame\tgames\tgood\tplay\tteam\tthink\twin\tyear\n",
      "Tag 16 \t10\t11\t12\t13\t14\t15\t17\t18\t20\t25\n",
      "Tag 17 \tdoes\tdon\tedu\tgood\tjust\tknow\tlike\tsoon\tthink\tway\n",
      "Tag 18 \tcard\tcomputer\tdrive\thard\tmemory\tmonitor\tneed\tthanks\tuse\tvideo\n",
      "Tag 19 \tbelieve\tbible\tchristian\tdoes\tfact\tgod\tjesus\tlife\tpeople\tsay\n",
      "Tag 20 \tfile\tfiles\thelp\tprogram\tsoftware\tthanks\tuse\tusing\tversion\twindows\n"
     ]
    }
   ],
   "source": [
    "result = np.argsort(n_k_w, axis=1)[:, -10:]\n",
    "for i in range(20):\n",
    "    matrix = np.zeros((1, X_train.shape[1]))\n",
    "    for j in result[i]:\n",
    "        matrix[0, j] = 1\n",
    "    print('Tag {} \\t{}'.format(i + 1, '\\t'.join(vectorizer.inverse_transform(matrix)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84017c25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
